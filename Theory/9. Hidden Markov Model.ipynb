{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hidden Markov Model\n",
    "$$\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}\\limits}$$\n",
    "$$\\newcommand{\\argmax}{\\mathop{\\mathrm{argmax}}\\limits}$$\n",
    "ì´ë²ˆ PostëŠ” ë¬¸ì¼ì²  êµìˆ˜ë‹˜ì˜ ë¨¸ì‹ ëŸ¬ë‹ ë³´ë‹¤ëŠ” ì‹¤ì œ Problemì— ì ‘ëª©ì‹œì¼œì„œ Hidden Markov Modelì— ëŒ€í•˜ì—¬ ì•Œì•„ë³´ê³ , ì‹¤ì œ Modelì„ Codeë¡œì„œ í™•ì¸í•˜ëŠ” Postì…ë‹ˆë‹¤. (ë§ì€ ì±…ì—ì„œ CodeëŠ” ë‹¤ë£¨ì§€ ì•Šì•„ì„œ ë‚˜ì¤‘ì— ì‚¬ìš©í•˜ê¸° ìœ„í•˜ì—¬ ì •ë¦¬í•˜ì˜€ìŠµë‹ˆë‹¤.)\n",
    "\n",
    "- 9.1 What is Hidden Markov Model?\n",
    "- 9.2 Viterbi Decoding Algorithm\n",
    "- 9.3 Forward-Backward probability Cacluation\n",
    "- 9.4 Baum-Welch Algorithm\n",
    "- 9.5 Hidden Markov Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 What is Hidden Markov Model?\n",
    "**HMM(Hidden Markov Model)ì´ë¼ëŠ” ê²ƒì€ Dataë¥¼ ê°€ì§€ê³  Hiddenì¸ Stateë¥¼ ì¸¡ì •í•˜ëŠ” Algorithmì´ë‹¤.**  \n",
    "\n",
    "ì‹¤ì œ ì´ëŸ¬í•œ ì„¤ëª…ë§Œìœ¼ë¡œëŠ” ì™€ë‹¿ì§€ ì•Šìœ¼ë‹ˆ ì˜ˆì œë¡œ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n",
    "ì–´ë– í•œ Dataê°€ ATCGA... ê°™ì€ Dataê°€ ê´€ì¸¡ë˜ì—ˆë‹¤ê³  í•˜ì.  \n",
    "ê°ê°ì˜ Data A or T or C or GëŠ” q0 or q1 or q2 or q3 or q4ì˜ Stateë¼ê³  ê°€ì •í•˜ì.  \n",
    "\n",
    "ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” X(=ATCGA...)ë§Œ Observationí•˜ì—¬ì„œ Latent Varialbesì¸ Stateë¥¼ ì¸¡ì •í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.  \n",
    "\n",
    "ì´ì „ Post <a href=\"\">8. K-Means Clustering and Gaussian Mixture Model\n",
    "</a>ì™€ ê°™ì´ Latent Variablesë¥¼ ì¸¡ì •í•´ì•¼ í•˜ë¯€ë¡œ EM Algorithmìœ¼ë¡œì„œ í•´ê²°í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "ì•Œ ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ì€ StateëŠ” Intron, Exonì´ ì¡´ì¬í•˜ê²Œ ë˜ê³ , ê°ê°ì˜ Stateì˜ Emissionì€ A,T,C,Gê°€ ìˆë‹¤ëŠ” ê²ƒ ì´ë‹¤.  \n",
    "\n",
    "ì•ìœ¼ë¡œ ì‹¤ì œ ì ‘ê·¼í•  Exampleì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n",
    "![png](./images/25.png)\n",
    "\n",
    "ìœ„ì˜ ë¬¸ì œì— ë§ê²Œ ì•ìœ¼ë¡œì˜ ì‹ì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•  Notationì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "- ğ´=<span>$a_{ij}$</span>: i ë²ˆì§¸ Stateì—ì„œ jë²ˆì§¸ Stateë¡œ ë„˜ì–´ê°ˆ í™•ë¥  => Transition Probability\n",
    "- ğµ=<span>$b_i(o_t)$</span>: i ë²ˆì§¸ Stateì—ì„œÂ ğ‘œğ‘¡ê°€ Emissionë  í™•ë¥  => Emission Probability\n",
    "- ğ‘‚=[ğ´,ğ‘‡,ğ¶,ğº,ğ‘‡,ğ´]: ê´€ì¸¡ëœ Data -> Length: 6\n",
    "- ğ‘„=[ğ‘0,ğ‘1,ğ‘2,ğ‘3,ğ‘4]: ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” Status\n",
    "\n",
    "ëª¨ë“  í™•ë¥ ì€ Conditionalë¡œì„œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.  \n",
    "ì¦‰, ìƒê°í•´ë³´ë©´ ê°ê°ì˜ A,Bë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìƒê°í•  ìˆ˜ ìˆë‹¤.  \n",
    "\n",
    "- <span>$a_{ij} = P(j|i)$</span>: í˜„ì¬ i Stateì¼ë•Œ ë‹¤ìŒ Stateê°€ jì¼ í™•ë¥ \n",
    "- <span>$b_i(o_t) = P(o|i)$</span>: í˜„ì¬ i Stateì¼ë•Œ oë¥¼ Emissioní•  í™•ë¥ \n",
    "\n",
    "ì¦‰, Conditional Probabilityë¡œì„œ ëª¨ë“  ê²ƒì„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Viterbi Decoding Algorithm\n",
    "ë¨¼ì € ìµœì¢…ì ì¸ Viterbi Algorithmì„ ì‚´í´ë³´ë©´ Notationì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬ ëœë‹¤.\n",
    "\n",
    "- <span>$ğ‘‰_t(ğ‘—)=max_i[ğ‘‰_tâˆ’1(ğ‘–)ğ‘_{ij}ğ‘_j]$<span>: Viterbi Algorithm => të²ˆì§¸ ì‹œì ì—ì„œ jë²ˆì§¸ ì€ë‹‰ ìƒíƒœê°€ ê´€ì¸¡ë˜ê³  ê´€ì¸¡ì¹˜Â ğ‘‚ğ‘¡(=A or T or C or G) ê°€ ê´€ì¸¡ë  í™•ë¥ \n",
    " - j=0: A\n",
    " - j=1: T\n",
    " - j=2: C\n",
    " - j=3: G\n",
    "- <span>$b_t(j)= \\argmax_i[V_{tâˆ’1}(ğ‘–)âˆ—a_{ij}âˆ—b_j(o_t)]$</span>: Traceback => í™•ë¥ ì´ ë†’ì€ Statusë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ Traceback\n",
    "    \n",
    "í˜„ì¬ ì‹¤ì œ DataëŠ” ATCGTAê°€ ê´€ì¸¡ë˜ì—ˆë‹¤. ê°ê°ì˜ Stateë¡œ ë„˜ì–´ê°ˆ í™•ë¥ ì´ë‘, ê°ê°ì˜ Stateì—ì„œ Emissionë  í™•ë¥ ì´ ì¡´ì¬í•˜ë¯€ë¡œ, ì´ëŸ¬í•œ Sequenceê°€ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ê²½ë¡œë¥¼ ìƒê°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.(ê°ˆ ìˆ˜ ì—†ëŠ” ê³³ì€ ì œì™¸í•œë‹¤.)  \n",
    "\n",
    "Viterbi Algorithmê°’ì„ ìƒê°í•´ë³´ë©´, i->jê°€ ë ìˆ˜ ìˆëŠ” ëª¨ë“  Transmission Probabilityì™€ ië²ˆì§¸ì˜ ê°ê°ì˜ Stateì—ì„œ Emissionë  Probabilityì˜ ê³± ì¤‘ ê°€ì¥ í° ê°’ì„ ì„ íƒí•˜ê²Œ ëœë‹¤. ë”°ë¼ì„œ ê°€ì¥ ë†’ì„ í™•ë¥ ì„ ì„ íƒí•˜ê²Œ ë˜ë©´, Data Sequenceì— ë§ëŠ” í™•ë¥ ì´ ë†’ì€ Stateë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆë‹¤.  \n",
    "    \n",
    "Tracebackì„ ì‚´í´ë³´ê²Œ ë˜ë©´, Viterbi Algorithmì€ MAXê°’ì„ ì„ íƒí•˜ë¯€ë¡œ ê·¸ ê°’ì„ ì–´ë””ì—ë‹¤ê°€ ì €ì¥í•´ë‘ë©´, Argmaxë¥¼ í†µí•˜ì—¬ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê³³ìœ¼ë¡œì„œ Tracebackì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒ ì´ë‹¤.\n",
    "\n",
    "    \n",
    "![png](./images/26.png)\n",
    "    \n",
    "ìœ„ì˜ ê·¸ë¦¼ì„ Matrixë¡œì„œ í‘œí˜„í•˜ê¸° ìœ„í•˜ì—¬ ê°ê°ì˜Â ğ‘‰ğ‘–(ğ‘—)ë¥¼ ê³„ì‚°í•˜ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Viterbi Algorithmì‹ì€ Maxë¥¼ ì‚¬ìš©í•˜ì—¬ì•¼ í•˜ë‚˜, q2ë¥¼ ì˜ˆì‹œë¡œ í•˜ë©´, q1 -> q2ëŠ” ì²˜ìŒë§Œ ê°€ëŠ¥í•˜ê³ , q1 -> q2, q3 -> q2, q4 -> q2ëŠ” ë¶ˆê°€ëŠ¥ í•©ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ q3ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ maxë¡œì„œ ê°’ì„ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ê²½ìš°ì˜ ìˆ˜ê°€ í•˜ë‚˜ë§Œ ê°€ëŠ¥í•œ ìƒíƒœë¡œ ì‹ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.)\n",
    "\n",
    "ğ‘‰1(1) = ğ‘01âˆ—ğ‘0(0) =1âˆ—0.1=0.1\n",
    "\n",
    "ğ‘‰2(2)=ğ‘‰1(1)âˆ—ğ‘12âˆ—ğ‘2(1)=0.1âˆ—0.5âˆ—0.25=0.0125\n",
    "ğ‘‰2(3)=ğ‘‰1(1)âˆ—ğ‘13âˆ—ğ‘3(1)=0.1âˆ—0.5âˆ—0.17=0.0085\n",
    "\n",
    "ğ‘‰3(2)=ğ‘‰2(2)âˆ—ğ‘22âˆ—ğ‘2(2)=0.0125âˆ—0.65âˆ—0.15=0.00121875\n",
    "ğ‘‰3(3)=ğ‘‰2(3)âˆ—ğ‘33âˆ—ğ‘3(2)=0.0085âˆ—0.8âˆ—0.43=0.002924\n",
    "ğ‘‰3(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰2(2)âˆ—ğ‘24âˆ—ğ‘4(2),ğ‘‰2(3)âˆ—ğ‘34âˆ—ğ‘4(2),0]=ğ‘šğ‘ğ‘¥[0,0.0009625,0.000374,0]=0.0009625\n",
    "ğ‘‰4(2)=ğ‘‰3(2)âˆ—ğ‘22âˆ—ğ‘(3)=0.00121875âˆ—0.65âˆ—0.25=0.000198047\n",
    "ğ‘‰4(3)=ğ‘‰3(3)âˆ—ğ‘33âˆ—ğ‘3(3)=0.002924âˆ—0.8âˆ—0.29=0.000678368\n",
    "ğ‘‰4(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰3(2)âˆ—ğ‘24âˆ—ğ‘4(3),ğ‘‰3(3)âˆ—ğ‘34âˆ—ğ‘4(3),0]=ğ‘šğ‘ğ‘¥[0,0.000157828,0.000216376,0]=0.000216376\n",
    "\n",
    "ğ‘‰5(2)=ğ‘‰4(2)âˆ—ğ‘22âˆ—ğ‘2(1)=0.000198047âˆ—0.65âˆ—0.25=0.000032183\n",
    "ğ‘‰5(3)=ğ‘‰4(3)âˆ—ğ‘33âˆ—ğ‘3(1)=0.000678368âˆ—0.8âˆ—0.17=0.000092258\n",
    "ğ‘‰5(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰4(2)âˆ—ğ‘24âˆ—ğ‘4(1),ğ‘‰4(3)âˆ—ğ‘34âˆ—ğ‘4(1),0]=ğ‘šğ‘ğ‘¥[0,0.00009704,0.000018997,0]=0.000018997\n",
    "\n",
    "ğ‘‰6(2)=ğ‘‰5(2)âˆ—ğ‘22âˆ—ğ‘2(0)=0.000032183âˆ—0.65âˆ—0.35=0.000007322\n",
    "ğ‘‰6(3)=ğ‘‰5(3)âˆ—ğ‘33âˆ—ğ‘3(0)=0.000092258âˆ—0.8âˆ—0.11=0.000008119\n",
    "ğ‘‰6(4)=ğ‘šğ‘ğ‘¥[0,ğ‘‰5(2)âˆ—ğ‘24âˆ—ğ‘4(0),ğ‘‰5(3)âˆ—ğ‘34âˆ—ğ‘4(0),0]=ğ‘šğ‘ğ‘¥[0,0.000003481,0.000004982,0]=0.000004982\n",
    "\n",
    "ì‹¤ì œ ê³„ì‚°í•œ ê°’ì„ Matrixë¡œì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n",
    "    \n",
    "![png](./images/27.png)\n",
    "    \n",
    "Trace Backì„ ìˆ˜í–‰í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n",
    "- End:Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0,0,0,1âˆ—.000004982]=3=ğ‘4\n",
    "- End-1: ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0,0.000003481,0.000004982,0]=q2\n",
    "- End-2:Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0,0,0.000092258,0]=2=ğ‘3\n",
    "\n",
    "    ...\n",
    "    \n",
    "- Start + 1:Â ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥[0.1,0,0,0]=0=ğ‘1\n",
    "\n",
    "ë”°ë¼ì„œ Tracebackì˜ ê²°ê³¼ë¡œ ì¸í•˜ì—¬ Stateê°€ ë³€í•œ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n",
    "ğ‘0â†’ğ‘1â†’ğ‘3â†’ğ‘3â†’ğ‘3â†’ğ‘3â†’ğ‘4â†’ğ‘0  \n",
    "    \n",
    "ìœ„ì˜ ê³¼ì •ì„ Matrixì— ì—°ê´€ì§€ì–´ ìƒê°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œí•  ìˆ˜ ìˆë‹¤.\n",
    "![png](./images/28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Forward-Backward probability Cacluation\n",
    "**Hidden Markov Modelì˜ ì „ë°˜ì ì¸ ë‚´ìš©ê³¼ Stateë¥¼ ì•Œì•„ë‚¼ ìˆ˜ ìˆëŠ” Viterbi Algorithmì˜ ê²½ìš°ì—ëŠ” ê°„ë‹¨í•˜ë¯€ë¡œ ì‹¤ì œ Dataì— ì ìš©ì„ í•˜ì—¬ ì•Œì•„ë³´ì•˜ë‹¤.**  \n",
    "\n",
    "Forward-Backwardì™€ Baum-Welch Algorithmì˜ ê²½ìš°ì—ëŠ” Modelì„ ì‹¤ì§ˆì ìœ¼ë¡œ Trainningí•˜ëŠ” ë¶€ë¶„ì´ë¯€ë¡œ ì¢€ ë” Genearlí•œ ìƒíƒœì˜ ìˆ˜ì‹ì„ ìœ ë„í•´ê°€ë©° ì•Œì•„ë³´ì. (ì´ì „ì— ì‚¬ìš©í•œ Notationì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.)  \n",
    "\n",
    "ê°ê°ì˜ Forward, Backward ProbabilityëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œì‹œí•©ë‹ˆë‹¤.\n",
    "- Forward Probability: <span>$\\alpha_t(j) = \\sum_{i=1}^{n} \\alpha_{t-1}a_{ij}b_j(o_t)$</span>\n",
    "- Backward Probability: <span>$\\beta_t(i) = \\sum_{j=1}^{n} \\beta_{t+1}(j)a_{ij}b_j(o_{t+1})$</span>\n",
    "\n",
    "**Viterbi Algorithmì˜ ì‹ì¸ <span>$ğ‘‰_t(ğ‘—)=max_i[ğ‘‰_tâˆ’1(ğ‘–)ğ‘_{ij}ğ‘_j]$</span>ì™€ ë¹„êµí•˜ê²Œ ë˜ë©´, Viterbi Algorithmì€ Maxê°’ì„ ì°¾ìœ¼ë¯€ë¡œ Indexingì„ í†µí•˜ì—¬ TraceBackì´ ê°€ëŠ¥í•˜ì˜€ë‹¤ë©´, Forward-Backward ProbabilityëŠ” ëª¨ë“  í™•ë¥ ì„ Summationí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— TraceBackì´ ë¶ˆê°€ëŠ¥ í•˜ë‹¤. í•˜ì§€ë§Œ, Summationì´ë¯€ë¡œ ì´ë¥¼ í™œìš©í•˜ì—¬ ê°ê°ì˜ í™•ë¥ ì— ëŒ€í•˜ì—¬ Updateê°€ ê°€ëŠ¥í•˜ë‹¤.**  \n",
    "\n",
    "ìµœì¢…ì ìœ¼ë¡œ Modelì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´(9-5 Code) Forward-Backward Probabilityë¥¼ ì‚¬ìš©í•œ Baum-Welch Algorithmìœ¼ë¡œì„œ Updateë¥¼ í•˜ê²Œ ë˜ê³ , Viterbi Algorithmìœ¼ë¡œì„œ Modelì„ í‰ê°€í•˜ê²Œ ëœë‹¤.\n",
    "\n",
    "ì „ë°©í™•ë¥ (Forward Probability)ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "<img src=\"https://i.imgur.com/mbBaTch.png\"><br>\n",
    "ì‚¬ì§„ ì°¸ì¡°: <a href=\"https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/\">ratsgo ë¸”ë¡œê·¸</a><br>\n",
    "<p>$${ \\alpha  }_{ 3 }(4)=\\sum _{ i=1 }^{ 4 }{ { \\alpha  }_{ 2 }(i)\\times { a }_{ i4 } } \\times { b }_{ 4 }({ o }_{ 3 })$$</p>\n",
    "\n",
    "í›„ë°©í™•ë¥ (Backward Probability)ì˜ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "<img src=\"https://i.imgur.com/bP9BdJy.png\"><br>\n",
    "ì‚¬ì§„ ì°¸ì¡°: <a href=\"https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/\">ratsgo ë¸”ë¡œê·¸</a><br>\n",
    "<p>$${ \\beta  }_{ 3 }(4)=\\sum _{ j=1 }^{ 4 }{ { a }_{ 4j } } \\times { b }_{ j }({ o }_{ 4 })\\times { \\beta  }_{ 4 }(j)$$</p>\n",
    "\n",
    "**ì´ ë‘í™•ë¥ ì„ ê³±í•˜ë©´ íŠ¹ì • Nodeë¥¼ ì§€ë‚˜ëŠ” ëª¨ë“  Probabilityë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.**  \n",
    "\n",
    "ì‚¬ì§„ìœ¼ë¡œì„œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  \n",
    "<img src=\"https://i.imgur.com/3SQDk3b.png\"><br>\n",
    "ì‚¬ì§„ ì°¸ì¡°: <a href=\"https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/\">ratsgo ë¸”ë¡œê·¸</a><br>\n",
    "<p>$${ \\alpha  }_{ t }\\left( j \\right) \\times { \\beta  }_{ t }\\left( j \\right) =P\\left( { q }_{ t }=j,O|\\theta  \\right)$$</p>\n",
    "\n",
    "ìœ„ì˜ ìˆ˜ì‹ì„ í™œìš©í•˜ë©´ HMMì˜ ëª¨ë“  í™•ë¥ ì— ëŒ€í•´ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤.(Start StateëŠ” q0ë¼ê³  ìƒê°í•œë‹¤ë©´)  \n",
    "<p>$$P(O|\\theta) = \\sum_{i=1}^{n} \\alpha_t(s)\\beta_t(s) = P(q_t=q_0,O | \\theta) = \\beta_o(q_0)$$</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Baum-Welch Algorithm\n",
    "ìš°ë¦¬ëŠ” ìœ„ì—ì„œ <span>${ \\alpha  }_{ t }\\left( j \\right) \\times { \\beta  }_{ t }\\left( j \\right) =P\\left( { q }_{ t }=j,O|\\theta  \\right)$</span>ì‹ì„ ì–»ì—ˆë‹¤.  \n",
    "\n",
    "ì˜ ìƒê°í•´ë³´ë…€ Baum-Welch Algorithmì€ EM Algorithmì´ë‹¤.  \n",
    "Latente Variableì¸ Stateë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•˜ì—¬ Forward, Backward ê°’ì„ ê³„ì‚°í•˜ëŠ” ë‹¨ê³„ê°€ E-Stepì´ê³  ì´ëŸ¬í•œ ê°’ì„ í™œìš©í•˜ì—¬ A,B,Initial Probabilityë¥¼ Updateí•˜ê¸° ë•Œë¬¸ì´ë‹¤.  \n",
    "\n",
    "**M-Step**  \n",
    "**1. Emission Probability**  \n",
    "íŠ¹ì • tì‹œì ì—ì„œ Observationì´ jì¼ í™•ë¥ ì€ ë§¤ìš° ê³„ì‚°í•˜ê¸° ì‰½ë‹¤.  \n",
    "<p>$$\\gamma_t(j) = P\\left( { q }_{ t }=j|O,\\theta  \\right)$$</p>\n",
    "<p>$$= \\frac{P\\left( { q }_{ t }=j,O|\\theta  \\right)}{P(O|\\theta)} = \\frac{\\alpha_{t}\\left( j \\right) \\times \\beta_{ t }\\left( j \\right)}{\\sum_{i=1}^{n} \\alpha_t(s)\\beta_t(s)}$$</p>\n",
    "ìœ„ì—ì„œ ë¯¸ë¦¬ êµ¬í•œ ì‹ìœ¼ë¡œì„œ í¸í•˜ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.  \n",
    "\n",
    "ìœ„ì˜ ì‹ì„ í™œìš©í•˜ì—¬ ì‹¤ì œ Emission Probabilityë¥¼ Updateí•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.  \n",
    "\n",
    "<p>$$\\hat{b_j}(v_k) = \\frac{\\sum_{t=1, s.t.o_t=v_k}^{T}\\gamma_t(j)}{\\sum_{t=1}^{T}\\gamma_t(j)}$$</p>\n",
    "\n",
    "**ìœ„ì˜ ì‹ì„ ì‚´í´ë³´ê²Œ ë˜ë©´, ëª¨ë“  Observationì—ì„œ Emission Probabilityë¥¼ ê³„ì‚°í•œ ê°’ê³¼ Modelì´ ì˜ˆì¸¡í•œ Observationì´ ì‹¤ì œ Observationì´ ê°™ì€ ë•Œì˜ í™•ë¥ ë¡œì„œ ë‚˜íƒ€ë‚´ê²Œ ëœë‹¤.**  \n",
    "\n",
    "**2. Transmission Probability**  \n",
    "Transmissionì¸ ê²½ìš°ì—ëŠ” í•œê°€ì§€ ë” ìƒê°í•´ì•¼ í•˜ëŠ” ì ì´ ìˆë‹¤. íŠ¹ì • ì‹œì ì—ì„œì˜ Emissionì´ <span>${ q }_{ t }=i$</span>ì¸ ê²½ìš°ì— <span>${ q }_{ t +1}=j$</span>ì´ ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—ë‹¤. ì¦‰, í˜„ì¬ ì¸¡ì •í•˜ê³ ìí•˜ëŠ” tì‹œì ì—ì„œ ë‹¤ìŒ ì‹œì ê¹Œì§€ ìƒê°í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.  \n",
    "\n",
    "ë”°ë¼ì„œ ìœ„ì—ì„œ êµ¬í•œ ì‹ì—ì„œ <span>$a_{ij}b_{j}(o_t)$</span>ë¥¼ ê³±í•´ì£¼ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒ ì´ë‹¤. ì´ë¥¼ ì‹ìœ¼ë¡œì„œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  \n",
    "<p>$$% <![CDATA[\n",
    "\\begin{align*}\n",
    "{ \\xi  }_{ t }\\left( i,j \\right) =&\\frac { P\\left( { q }_{ t }=i,{ q }_{ t+1 }=j,O|\\lambda  \\right)  }{ P\\left( O|\\lambda  \\right)  } \\\\ =&\\frac { { \\alpha  }_{ t }\\times { a }_{ ij }\\times { b }_{ j }\\left( { o }_{ t+1 } \\right) \\times { \\beta  }_{ t+1 }\\left( j \\right)  }{ \\sum _{ s=1 }^{ n }{ \\alpha _{ t }\\left( s \\right) \\times \\beta _{ t }\\left( s \\right)  }  }\n",
    "\\end{align*} %]]>$$</p>\n",
    "\n",
    "ìœ„ì˜ ìˆ˜ì‹ì„ í™œìš©í•˜ì—¬ Transmission Probabilityë¥¼ Updateí•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.  \n",
    "<p>$$\\hat { a } _{ ij }=\\frac { \\sum _{ t=1 }^{ T-1 }{ { \\xi  }_{ t }\\left( i,j \\right)  }  }{ \\sum _{ t=1 }^{ T-1 }{ \\sum _{ k=1 }^{ N }{ { \\xi  }_{ t }\\left( i,k \\right)  }  }  }$$</p>\n",
    "\n",
    "**ìœ„ì˜ ì‹ì„ ì‚´í´ë³´ê²Œ ë˜ë©´ iì‹œì ì—ì„œ jë²ˆì§¸ ì‹œì ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆëŠ” ëª¨ë“  Transimission Probabilityì—ì„œ ì‹¤ì œ í™•ë¥ ë¡œì„œ ê°„ Transmission Probabilityì˜ í™•ë¥ ë¡œì„œ ë‚˜íƒ€ë‚¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.**  \n",
    "\n",
    "ìœ„ì˜ ë‘ê°€ì§€ì— ëŒ€í•œ ëª¨ë“  ìì„¸í•œ ìˆ˜ì‹ì€ <a href=\"https://wjddyd66.github.io/machine%20learning/Theory(8)K-Means-Clustering-and-Gaussian-Mixture-Model(3)/\">EM-Algorithm</a>ì„ ì‚¬ìš©í•˜ê³ , <a href=\"https://kooc.kaist.ac.kr/machinelearning2__17/lecture/10872/\">ë¬¸ì¼ì²  êµìˆ˜ë‹˜ ê°•ì˜</a>ì—ì„œ ìì„¸í•œ ìœ ë„ë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Hidden Markov Code\n",
    "ì‹¤ì œ Packageë¡œì„œ hmm learn(https://hmmlearn.readthedocs.io/en/latest/)ë¥¼ ì œê³µí•˜ë‚˜ ì‹œë„í•´ë³´ê³ ì í•˜ëŠ” Datasetì´ ì ì–´ì„œ ì˜ ì‘ë™í•˜ì§€ ì•Šì•˜ë‹¤.  \n",
    "\n",
    "ë”°ë¼ì„œ Low Levelì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” Implement Viterbi Algorithm in Hidden Markov Model using Python and R(http://www.adeveloperdiary.com/data-science/machine-learning/implement-viterbi-algorithm-in-hidden-markov-model-using-python-and-r/)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition\n",
      "[[0.         0.5        0.5       ]\n",
      " [0.02941173 0.48529414 0.48529414]\n",
      " [0.02941173 0.48529414 0.48529414]]\n",
      "\n",
      "Emssion\n",
      "[[1.28376738e-27 2.60099443e-23 1.86212486e-23 3.11269808e-28\n",
      "  1.00000000e+00]\n",
      " [2.64705873e-01 2.94117637e-01 2.05882346e-01 2.35294109e-01\n",
      "  3.51426358e-08]\n",
      " [2.64705873e-01 2.94117637e-01 2.05882346e-01 2.35294109e-01\n",
      "  3.51426358e-08]]\n",
      "\n",
      "Accuracy 0.6388888888888888\n",
      "Precision 0.6176470588235294\n",
      "['q0', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q1', 'q0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in log\n",
      "/root/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:78: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(30)\n",
    "\n",
    "def forward(V, a, b, initial_distribution):\n",
    "    alpha = np.zeros((V.shape[0], a.shape[0]))\n",
    "    alpha[0, :] = initial_distribution * b[:, V[0]]\n",
    " \n",
    "    for t in range(1, V.shape[0]):\n",
    "        for j in range(a.shape[0]):\n",
    "            # Matrix Computation Steps\n",
    "            #                  ((1x2) . (1x2))      *     (1)\n",
    "            #                        (1)            *     (1)\n",
    "            alpha[t, j] = alpha[t - 1].dot(a[:, j]) * b[j, V[t]]\n",
    " \n",
    "    return alpha\n",
    " \n",
    "\n",
    "def backward(V, a, b):\n",
    "    beta = np.zeros((V.shape[0], a.shape[0]))\n",
    " \n",
    "    # setting beta(T) = 1\n",
    "    beta[V.shape[0] - 1] = np.ones((a.shape[0]))\n",
    " \n",
    "    # Loop in backward way from T-1 to\n",
    "    # Due to python indexing the actual loop will be T-2 to 0\n",
    "    for t in range(V.shape[0] - 2, -1, -1):\n",
    "        for j in range(a.shape[0]):\n",
    "            beta[t, j] = (beta[t + 1] * b[:, V[t + 1]]).dot(a[j, :])\n",
    " \n",
    "    return beta\n",
    " \n",
    "\n",
    "def baum_welch(V, a, b, initial_distribution, n_iter=100):\n",
    "    M = a.shape[0]\n",
    "    T = len(V)\n",
    " \n",
    "    for n in range(n_iter):\n",
    "        alpha = forward(V, a, b, initial_distribution)\n",
    "        beta = backward(V, a, b)\n",
    " \n",
    "        xi = np.zeros((M, M, T - 1))\n",
    "        for t in range(T - 1):\n",
    "            denominator = np.dot(np.dot(alpha[t, :].T, a) * b[:, V[t + 1]].T, beta[t + 1, :])\n",
    "            for i in range(M):\n",
    "                numerator = alpha[t, i] * a[i, :] * b[:, V[t + 1]].T * beta[t + 1, :].T\n",
    "                xi[i, :, t] = numerator / denominator\n",
    " \n",
    "        gamma = np.sum(xi, axis=1)\n",
    "        a = np.sum(xi, 2) / np.sum(gamma, axis=1).reshape((-1, 1))\n",
    " \n",
    "        # Add additional T'th element in gamma\n",
    "        gamma = np.hstack((gamma, np.sum(xi[:, :, T - 2], axis=0).reshape((-1, 1))))\n",
    " \n",
    "        K = b.shape[1]\n",
    "        denominator = np.sum(gamma, axis=1)\n",
    "        for l in range(K):\n",
    "            b[:, l] = np.sum(gamma[:, V == l], axis=1)\n",
    " \n",
    "        b = np.divide(b, denominator.reshape((-1, 1)))\n",
    " \n",
    "    return (a, b)\n",
    " \n",
    "\n",
    "def viterbi(V, a, b, initial_distribution):\n",
    "    T = V.shape[0]\n",
    "    M = a.shape[0]\n",
    " \n",
    "    omega = np.zeros((T, M))\n",
    "    omega[0, :] = np.log(initial_distribution * b[:, V[0]])\n",
    " \n",
    "    prev = np.zeros((T - 1, M))\n",
    " \n",
    "    for t in range(1, T):\n",
    "        for j in range(M):\n",
    "            # Same as Forward Probability\n",
    "            probability = omega[t - 1] + np.log(a[:, j]) + np.log(b[j, V[t]])\n",
    " \n",
    "            # This is our most probable state given previous state at time t (1)\n",
    "            prev[t - 1, j] = np.argmax(probability)\n",
    " \n",
    "            # This is the probability of the most probable state (2)\n",
    "            omega[t, j] = np.max(probability)\n",
    " \n",
    "    # Path Array\n",
    "    S = np.zeros(T)\n",
    " \n",
    "    # Find the most probable last hidden state\n",
    "    last_state = np.argmax(omega[T - 1, :])\n",
    " \n",
    "    S[0] = last_state\n",
    " \n",
    "    backtrack_index = 1\n",
    "    for i in range(T - 2, -1, -1):\n",
    "        S[backtrack_index] = prev[i, int(last_state)]\n",
    "        last_state = prev[i, int(last_state)]\n",
    "        backtrack_index += 1\n",
    " \n",
    "    # Flip the path array since we were backtracking\n",
    "    S = np.flip(S, axis=0)\n",
    " \n",
    "    # Convert numeric values to actual hidden states\n",
    "    result = []\n",
    "    for s in S:\n",
    "        if s == 0:\n",
    "            result.append(\"q0\")\n",
    "        elif s==1:\n",
    "            result.append(\"q1\")\n",
    "        elif s==2:\n",
    "            result.append(\"q2\")\n",
    " \n",
    "    return result\n",
    " \n",
    "\n",
    "    \n",
    "data = pd.read_csv('./data.csv')\n",
    " \n",
    "V = data['Visible'].values\n",
    " \n",
    "# Transition Probabilities\n",
    "a = np.ones((3, 3))\n",
    "a = a / np.sum(a, axis=1)\n",
    " \n",
    "# Emission Probabilities\n",
    "b = np.ones((3,5))\n",
    "b = b / np.sum(b, axis=1).reshape((-1, 1))\n",
    " \n",
    "# Equal Probabilities for the initial distribution\n",
    "initial_distribution = np.array((1.0, 0.0, 0.0))\n",
    " \n",
    "transition, emission = baum_welch(V, a, b, initial_distribution, n_iter=100)\n",
    "print('Transition')\n",
    "print(transition)\n",
    "print()\n",
    "\n",
    "print('Emssion')\n",
    "emission = emission / np.sum(emission, axis=1).reshape((-1, 1))\n",
    "print(emission)\n",
    "print()\n",
    "\n",
    "pred = viterbi(V, transition, emission , initial_distribution)\n",
    "\n",
    "count = 0  \n",
    "TP = 0  \n",
    "FP = 0  \n",
    "   \n",
    "for i,p in enumerate(pred):\n",
    "    if p == 'q1':\n",
    "        FP+=1\n",
    "        if p == data['Hidden'][i]:\n",
    "            FP-=1\n",
    "            TP+=1\n",
    "    if p == data['Hidden'][i]:\n",
    "        count+=1\n",
    "        \n",
    "print('Accuracy',count/len(data))\n",
    "print('Precision', TP/(TP+FP))  \n",
    "   \n",
    "print(pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì˜ ê²°ê³¼ë¥¼ ì‚´í´ë³´ê²Œ ë˜ë©´ AccuracyëŠ” 64%ì˜ ê²°ê³¼ë¥¼ ì–»ì—ˆìœ¼ë‚˜, ì²˜ìŒê³¼ ë§ˆì§€ë§‰ì„ q0ë¼ê³  íŒë‹¨í•˜ê³ , ê·¸ ì™¸ì—ëŠ” q1ìœ¼ë¡œ íŒë‹¨í•˜ê²Œ ëœë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” Initializationì„ ì˜ëª» í•˜ì˜€ë‹¤ê³  ì¶”ì¸¡í•˜ì˜€ë‹¤. ì¦‰, AccuracyëŠ” ë†’ì•„ë„ Modelì˜ Precisionì€ ë§ì´ ë¶€ì¡±í•œ ìƒí™©ì´ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "**HMMì€ E-M Algorithmì´ë‹¤. ì¦‰, Local Minima or Maximaì— ë¹ ì§ˆ ìˆ˜ ìˆëŠ” ìƒí™©ì´ ì´ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ê²ƒì„ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ ê¸°ì¡´ì— ê°€ì§€ê³  ìˆëŠ” Dataë¥¼ ê°€ì§€ê³  Initializationê°’ì„ ê´€ì¸¡ëœ Dataì˜ MLEê°’ìœ¼ë¡œì„œ ë³€ê²½í•˜ì—¬ ê°’ì„ ì§€ì •í•  ìˆ˜ ìˆë‹¤.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLE of Emission Probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q0 MLE Probability\n",
      "[0.0, 0.0, 0.0, 0.0, 1.0]\n",
      "\n",
      "q1 MLE Probability\n",
      "[0.2857142857142857, 0.23809523809523808, 0.19047619047619047, 0.2857142857142857, 0.0]\n",
      "\n",
      "q2 MLE Probability\n",
      "[0.23076923076923078, 0.38461538461538464, 0.23076923076923078, 0.15384615384615385, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Emission Probability  \n",
    "q0 = data[data[\"Hidden\"]==\"q0\"]  \n",
    "q1 = data[data[\"Hidden\"]==\"q1\"]  \n",
    "q2 = data[data[\"Hidden\"]==\"q2\"]  \n",
    "\n",
    "q0_mle_list = []\n",
    "q1_mle_list = []\n",
    "q2_mle_list = []  \n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    q0_mle_list.append(len(q0[q0['Visible']==i])/len(q0))  \n",
    "\n",
    "    \n",
    "for i in range(5):\n",
    "    q1_mle_list.append(len(q1[q1['Visible']==i])/len(q1))  \n",
    "\n",
    "    \n",
    "for i in range(5):\n",
    "    q2_mle_list.append(len(q2[q2['Visible']==i])/len(q2))  \n",
    "\n",
    "    \n",
    "print('q0 MLE Probability')\n",
    "print(q0_mle_list)\n",
    "print()  \n",
    "\n",
    "\n",
    "print('q1 MLE Probability')\n",
    "print(q1_mle_list)\n",
    "print()  \n",
    "\n",
    "print('q2 MLE Probability')\n",
    "print(q2_mle_list)  \n",
    "emission_initial = np.stack((q0_mle_list,q1_mle_list,q2_mle_list),axis=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MLE of Transimission Probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transmission of Probability\n",
      "[[0.         1.         0.        ]\n",
      " [0.04761905 0.85714286 0.0952381 ]\n",
      " [0.         0.15384615 0.84615385]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Transimission Probability  \n",
    "transmission_array = np.array(((0,0,0),(0,0,0),(0,0,0)))  \n",
    "d = [\"q0\",\"q1\",\"q2\"]  \n",
    "\n",
    "for i in range(len(data)-1):\n",
    "    before = data[\"Hidden\"][i]\n",
    "    after = data[\"Hidden\"][i+1]  \n",
    "    \n",
    "    for i,value in enumerate(d):\n",
    "        for j,value2 in enumerate(d):\n",
    "            if before == value and after == value2:\n",
    "                transmission_array[i,j]+=1  \n",
    "\n",
    "                \n",
    "transmission_initial = transmission_array / np.sum(transmission_array, axis=1).reshape((-1, 1))  \n",
    "print('Transmission of Probability')  \n",
    "print(transmission_initial)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Viterbi Algorithm in Hidden Markov Model using Python with MLE Initial Probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition\n",
      "[[0.         1.         0.        ]\n",
      " [0.06640658 0.32785813 0.60573529]\n",
      " [0.         0.48157408 0.51842592]]\n",
      "\n",
      "Emssion\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00]\n",
      " [5.63565953e-01 4.81671036e-05 7.71107067e-05 4.36308770e-01\n",
      "  0.00000000e+00]\n",
      " [2.71049580e-02 5.27909937e-01 3.69502457e-01 7.54826478e-02\n",
      "  0.00000000e+00]]\n",
      "\n",
      "Accuracy 0.6111111111111112\n",
      "Precision 0.7058823529411765\n",
      "['q0', 'q1', 'q1', 'q2', 'q2', 'q1', 'q1', 'q1', 'q1', 'q2', 'q2', 'q1', 'q1', 'q2', 'q2', 'q1', 'q2', 'q1', 'q2', 'q1', 'q1', 'q2', 'q2', 'q1', 'q2', 'q2', 'q2', 'q1', 'q2', 'q2', 'q1', 'q1', 'q2', 'q2', 'q1', 'q0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:71: RuntimeWarning: divide by zero encountered in log\n",
      "/root/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:78: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "transition, emission = baum_welch(V, transmission_initial, emission_initial, initial_distribution, n_iter=100)\n",
    "print('Transition')\n",
    "print(transition)\n",
    "print()\n",
    "\n",
    "print('Emssion')\n",
    "emission = emission / np.sum(emission, axis=1).reshape((-1, 1))\n",
    "print(emission)\n",
    "print()\n",
    "\n",
    "pred = viterbi(V, transition, emission , initial_distribution) \n",
    "\n",
    "count = 0  \n",
    "TP = 0  \n",
    "FP = 0  \n",
    "   \n",
    "for i,p in enumerate(pred):\n",
    "    if p == 'q1':\n",
    "        FP+=1\n",
    "        if p == data['Hidden'][i]:\n",
    "            FP-=1\n",
    "            TP+=1\n",
    "    if p == data['Hidden'][i]:\n",
    "        count+=1\n",
    "        \n",
    "print('Accuracy',count/len(data))\n",
    "print('Precision', TP/(TP+FP))  \n",
    "   \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì˜ ê²°ê³¼ë¥¼ ì‚´í´ë³´ê²Œ ë˜ë©´, Accuracy(64% -> 61%)ëŠ” ë–¨ì–´ì¡Œìœ¼ë‚˜, í›¨ì”¬ ë” Precision(62% -> 70%)ì´ ë†’ì•„ì§„ ìƒí™©ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\n",
    "ìƒí™©ì— ë”°ë¼ì„œ, ë” ì¢‹ì€ Initializationì„ ì„ íƒí•˜ë©´ ë  ê²ƒì´ë‹¤.\n",
    "\n",
    "**Datasetì˜ ì ê³  Precisionì˜ ì¤‘ìš”ë„ì— ë”°ë¼ì„œ ìœ„ì˜ Modelì—ì„œ Initializationì„ ì–´ë–»ê²Œ í• ì§€ ì •í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ë°©ë²•ì´ë¼ê³  ìƒê°ëœë‹¤.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
