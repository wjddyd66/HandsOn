{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SVM\n",
    "$$\\newcommand{\\argmin}{\\mathop{\\mathrm{argmin}}\\limits}$$\n",
    "$$\\newcommand{\\argmax}{\\mathop{\\mathrm{argmax}}\\limits}$$\n",
    "Machine Learning의 기초적인 이론부분을 다시 제대로 잡고 싶어서 <a href=\"https://kaist.edwith.org/machinelearning1_17/joinLectures/9738\">문일철 교수님의 인공지능 및 기계학습 개론</a>을 정리한 Post입니다.\n",
    "\n",
    "- 5.1 Decision Boundary with Margin \n",
    "- 5.2 Maximizing the Margin\n",
    "- 5.3 Soft Margin with SVM\n",
    "- 5.4 Comparison to Logistic Regression\n",
    "- 5.5 Rethinking of SVM\n",
    "- 5.6 Primal and Dual with KKT Condition\n",
    "- 5.7 Kernel\n",
    "- 5.8 SVM with Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Decision Boundary with Margin\n",
    "SVM이란 Classification의 일종이다.  \n",
    "\n",
    "**SVM이란 이러한 상황에서 수많은 Decision Boundary에서 각각의 가장 가까운 Point 사이의 거리(Margin)을 최대화 하는 것 이다.**\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/609/0*lyr5-f7HRu34OLvd.png\"><br>\n",
    "사진 출처: <a href=\"https://medium.com/@unfinishedgod/r-%EC%8B%A0%EC%9A%A9%EB%B6%84%EC%84%9D-%EC%98%88%EC%B8%A1%EC%A0%81%EB%B6%84%EC%84%9D-%EC%84%9C%ED%8F%AC%ED%8A%B8-%EB%B2%A1%ED%84%B0-%EB%A8%B8%EC%8B%A0%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%AA%A8%EB%8D%B8%EB%A7%81-13e17af3e7ed\">medium.com</a>\n",
    "\n",
    "수식으로 나타내면 다음과 같다.  \n",
    "\n",
    "<p>$$f(x) = wx+b$$</p>\n",
    "Point가 x가로 하는 경우 Decision Boundary(Hyperplane)는 <span>$f(x) = wx+b=0$</span>으로 잡을 수 있다.  \n",
    "\n",
    "\n",
    "만약, Support Vector의 값이 <span>$f(x) = wx+b=a$</span>라면, Input Point에 따라서 <span>$a>0$</span>이면 Positive, <span>$a<0$</span>이면 Negative로서 판단할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Maximizing the Margin\n",
    "\n",
    "Margin을 Maximizing하기 위하여 먼저 Margin을 구하여보자.  \n",
    "먼저 위의 그림을 다시 한번 생각하고 용어를 정리하여 보자.\n",
    "- <span>$x_p$</span>: Hyperplane위의 Point\n",
    "- <span>$f(x) = wx+b = a \\text{  or  } -a$</span>: Support Vector\n",
    "- <span>$w$</span>: Hyperplane의 접선 Vector\n",
    "- <span>$r$</span>Hyperplane의 에서 Support Vector 까지의 거리\n",
    "\n",
    "위와 같이 가정하였을 경우 다음과 같이 x를 정의할 수 있다.\n",
    "<p>$$x=x_p + r\\frac{w}{||w||}$$</p>\n",
    "\n",
    "즉, 임의의 한 Point는 Hyperplane위의 Point를 기준으로 방향은 w, 크기는 r인 Vector로서 표현할 수 있는 것 이다.\n",
    "\n",
    "위의 식을 활용하면 다음과 같다.  \n",
    "<p>$$f(x) = w(x_p + r \\frac{w}{||w||})+b = r||w|| (\\because f(x_p) = wx_p+b = 0)$$</p>\n",
    "<p>$$\\therefore r = \\frac{f(x)}{||w||} \\rightarrow \\text{margin} = \\frac{2a}{||w||}$$</p>\n",
    "\n",
    "위에서 Support Vector의 값을 a라고 하였으므로 다음과 같이 표현할 수 있다.  \n",
    "<p>$$max_{w,b}2r = \\frac{2a}{||w||}$$</p>\n",
    "<p>$$s.t(wx_j+b)y_j \\ge a$$</p>\n",
    "\n",
    "위의 식에서 a는 임의의 상수이므로 a=1이라 가정하면 최종적인 Maximizing the Margin은 다음과 같이 나타낼 수 있다.  \n",
    "\n",
    "$$min_{w,b}||w||$$\n",
    "$$s.t.(wx_j+b)y_j \\ge 1$$\n",
    "\n",
    "이러한 방법은 Hard Margin이라고 불리게 된다. Hard Margin이란 Linear한 Line으로서 어떻게든 Data를 분류한다는 의미이다.  \n",
    "\n",
    "아래와 같은 Data Point 분포가 있을 경우 문제를 생각해 보자.  \n",
    "![png](./images/12.png)\n",
    "\n",
    "Decision Boundary와 Classify의 기준이 되는 Line사이의 Data Point가 들어오는 경우, 혹은 Linear한 Line으로는 도저히 분류할 수 없는 상황 등이 있다.  \n",
    "\n",
    "위와 같은 Error상황에서 크게 2가지의 해결방안이 있다.\n",
    "\n",
    "1. Soft Margin: Error를 어느정도 허용하고 Linear하게 분류한다.\n",
    "2. Kernel Trick: Non Linear하게 분류한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Error Handling in SVM\n",
    "위에서 두가지 방법에 대하여 다음과 같이 정의 할 수 있다.\n",
    "\n",
    "**1. Soft Margin**\n",
    "- **Admin there will be an \"error\"**\n",
    "- Represent the error in our problem formulation\n",
    "- Try to reduce the error as well\n",
    "\n",
    "**2. Kernel Trick**\n",
    "- **Make decision boundary**\n",
    "- more complex\n",
    "- Go to **non-linear**\n",
    "\n",
    "Soft Margin과 Kernel Trick둘다, 위에서 선언한 <span>$min_{w,b}||w||,  s.t(wx_j+b)y_j \\ge 1$</span>로서는 Error를 해결할 수 없으므로 **Error에 관한 값을 Penalty Function으로서 나타내고 Penalty Function의 값 또한 Minimize하는 값을 찾아서 Classify하게 한다는 것 이다.**  \n",
    "\n",
    "즉, 어느정도 Error는 허용하나 그 Error가 최소화되게 만든다는 의미이다.  \n",
    "\n",
    "Penalty Function을 선언하는 방법은 2가지가 있다.\n",
    "\n",
    "**Option 1**  \n",
    "<p>$$min_{w,b}||w||+C*(\\text{Num of Error})$$</p>\n",
    "<p>$$s.t(wx_j+b)y_j \\ge 1$$</p>\n",
    "위의 식을 살펴보게 되면 Error의 개수 * C(임의의 상수)만큼 Penalty를 준다는 것 이다.  \n",
    "이러한 Penalty Function은 0-1 Loss라고 부르게 된다.  \n",
    "**0-1 Loss란 Hyperplane까지는 Penalty가 없으나 반대쪽의 Support Vector를 넘어가는 순간 1의 Penalty를 주는 Penalty Function이다. 하지만, 이러한 Function은 거리와 상관없이 Count만으로서 Penalty를 주기 때문에 정확한 방법이라고 할 수 없다.**\n",
    "\n",
    "**Option2**  \n",
    "<p>$$min_{w,b}||w||+C\\sum_{j}\\xi_j$$</p>\n",
    "<p>$$s.t(wx_j+b)y_j \\ge 1-\\xi_j$$</p>\n",
    "<p>$$\\xi_j \\ge 0$$</p>\n",
    "<p>$$\\text{EX)  } \\xi_j =(1-(wx_j+b)y_j)_{+}$$</p>\n",
    "\n",
    "위의 식을 살펴보게 되면 <span>$\\xi$</span>는 Distance에 따라 <span>$\\xi_j$</span>만큼 Penalty를 주겠다는 의미이다.  \n",
    "**0-1 Loss에서의 문제점이였던, 거리에 따른 가중치를 주지 않는 것은 해결하였다. 이러한 Loss Function은 Hinge Loss라고 불린다. 하지만, C라는 Constant를 정의해야 하는 새로운 문제가 발생하게 되었다.**\n",
    "\n",
    "위의 Option1,2의 Penalty Function을 SVM에 적용하여 Visualizatio하면 다음과 같이 나타낼 수 있다.\n",
    "![png](./images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Soft Margin with SVM\n",
    "**Soft Margin**\n",
    "- **Admin there will be an \"error\"**\n",
    "- Represent the error in our problem formulation\n",
    "- Try to reduce the error as well\n",
    "\n",
    "Soft Margin과 Kernel Trick둘다, 위에서 선언한 <span>$min_{w,b}||w||,  s.t(wx_j+b)y_j \\ge 1$</span>로서는 Error를 해결할 수 없으므로 **Error에 관한 값을 Penalty Function으로서 나타내고 Penalty Function의 값 또한 Minimize하는 값을 찾아서 Classify하게 한다는 것 이다.**  \n",
    "\n",
    "즉, 어느정도 Error는 허용하나 그 Error가 최소화되게 만든다는 의미이다.  \n",
    "\n",
    "Penalty Function을 선언하는 방법은 2가지가 있다.\n",
    "\n",
    "**Option 1**  \n",
    "<p>$$min_{w,b}||w||+C*(\\text{Num of Error})$$</p>\n",
    "<p>$$s.t(wx_j+b)y_j \\ge 1$$</p>\n",
    "위의 식을 살펴보게 되면 Error의 개수 * C(임의의 상수)만큼 Penalty를 준다는 것 이다.  \n",
    "이러한 Penalty Function은 0-1 Loss라고 부르게 된다.  \n",
    "**0-1 Loss란 Hyperplane까지는 Penalty가 없으나 반대쪽의 Support Vector를 넘어가는 순간 1의 Penalty를 주는 Penalty Function이다. 하지만, 이러한 Function은 거리와 상관없이 Count만으로서 Penalty를 주기 때문에 정확한 방법이라고 할 수 없다.**\n",
    "\n",
    "**Option2**  \n",
    "<p>$$min_{w,b}||w||+C\\sum_{j}\\xi_j$$</p>\n",
    "<p>$$s.t(wx_j+b)y_j \\ge 1-\\xi_j$$</p>\n",
    "<p>$$\\xi_j =(1-(wx_j+b)y_j)_{+} \\rightarrow \\xi_j \\ge 0$$</p>\n",
    "\n",
    "위의 식을 살펴보게 되면 <span>$\\xi$</span>는 Distance에 따라 <span>$\\xi_j$</span>만큼 Penalty를 주겠다는 의미이다.  \n",
    "**0-1 Loss에서의 문제점이였던, 거리에 따른 가중치를 주지 않는 것은 해결하였다. 이러한 Loss Function은 Hinge Loss라고 불린다. 하지만, C라는 Constant를 정의해야 하는 새로운 문제가 발생하게 되었다.**\n",
    "\n",
    "위의 Option1,2의 Penalty Function을 SVM에 적용하여 Visualizatio하면 다음과 같이 나타낼 수 있다.\n",
    "![png](./images/13.png)\n",
    "\n",
    "\n",
    "**새로운 Constant C에 대하여 값을 변경시키면서 결과를 출력하면 다음과 같은 결과를 얻을 수 있다.**  \n",
    "![png](./images/14.png)\n",
    "\n",
    "**Data가 Linear하게 Classify되지 않는다고 가정하였을 경우, Penalty가 작으면 여전히 잘 분류하지 못하지만, 일정 크기 이상일 경우에는 어느정도의 Error를 감수하고 잘 분류하는 것을 살펴볼 수 있다. 이러한 C의 값은 무조건 크면 좋은 것이 아니라, 우리가 사용하고자 하는 Data의 신뢰도에 따라 Penalty를 줘야지 의미있는 Classification Model(SVM)을 설계할 수 있을 것 이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Comparison to Logistic Regression\n",
    "위의 SoftMargin SVM에서 LossFunction을 Hinge Loss로서 사용하였다.  \n",
    "\n",
    "이러한 LossFunction은 이전에 배웠던 Logistic Regression에서도 사용하였다.  \n",
    "<a href=\"\">4.3 Logistic Regression Parameter Approximation 1</a>에서 Logistic Regression식 을 생각해보자.\n",
    "\n",
    "<p>$$\\hat{\\theta} = \\argmax_{\\theta} \\sum_{i=1}^N log(P(Y_i|X_i ;\\theta)) = \\argmax_{\\theta}\\sum_{i=1}^{N}(Y_iX_i\\theta - log(1+e^{X_i\\theta}))$$</p>\n",
    "\n",
    "위의 식을 살펴보게 되면 LossFunction은 <span>$log(1+e^{X_i\\theta})$</span>가 되는 것을 살펴볼 수 있다.\n",
    "\n",
    "최종적인 0-1, Hinge, Log Loss는 다음과 같이 나타낸다.  \n",
    "<img src=\"http://fa.bianp.net/blog/static/images/2013/loss_functions.png\"><br>\n",
    "사진 출처: <a href=\"http://fa.bianp.net/blog/2013/loss-functions-for-ordinal-regression/\">fa.bianp</a>\n",
    "\n",
    "위의 Loss를 생각해보면 다음과 같은 의미를 가지고 있다.  \n",
    "**SVM의 경우 Hyperplane을 기준으로서 Prediction이 맞다면 Penalty가 0이다. 즉, 완벽히 Classification가능하다고 생각한다는 것 이다.**  \n",
    "\n",
    "**Logistic Regression의 Log Loss Function을 살펴보게 되면, 아주 잘못되어도 0이아닌 값을 가지게 된다.(0에 매우 가까울 것 이다.)즉, 아주 0에 가까운 확률로서 Prediction이 잘못되었다고 판단할 수 있다는 것 이다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Rethinking of SVM\n",
    "**Kernel Trick**\n",
    "- **Make decision boundary**\n",
    "- more complex\n",
    "- Go to **non-linear**\n",
    "\n",
    "Error를 해결하기 위하여 2번째 방법인 Kernel을 이용하는 방법이다.  \n",
    "Kernel하나의 예시를 식으로서 표현하면 다음과 같다.  \n",
    "<p>$$\\varphi(<x_1,x_2>) = <x_1, x_2, x_1^2, ..., x_1x_2^2>$$</p>\n",
    "\n",
    "**즉, Kernel Trick이란 Data의 Dimension을 늘리는 방법이다.**  \n",
    "\n",
    "Kernel Trick을 사용하기 위하여 Duality -> KKT Condition을 사용하고 이를 SVM에 적용하자.\n",
    "\n",
    "**Lagrange multiplier**\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/242B713357C84A3402\">\n",
    "\n",
    "$$L(x,y,\\lambda) = f(x,y) - \\lambda(g(x,y)-c)$$\n",
    "$$\\frac{\\partial f}{\\partial x} = \\lambda \\frac{\\partial g}{\\partial x}$$\n",
    "$$\\frac{\\partial f}{\\partial y} = \\lambda \\frac{\\partial g}{\\partial y}$$\n",
    "\n",
    "f(x,y): Maximum지점을 찾기 위한 함수, g(x,y) = c는 조건  \n",
    "\n",
    "위의 식에서 구하고자 하는 함수와 조건을 넣으면 다음과 같이 Object Function을 얻을 수 있다.  \n",
    "### Duality\n",
    "위에서 설명한 Lagrange Multiplier를 사용하여 Primal -> Duality문제로 변형하여 보자.  \n",
    "얻고자 하는 것은 object Function이 Minimize되는 것 이다.  \n",
    "이것은 다음과 같이 나타낼 수 있다.  \n",
    "\n",
    "**Step 1**  \n",
    "<p>$$min_{x} f(x)$$</p>\n",
    "<p>$$\\text{subject to  }g(x) \\le 0, h(x)=0$$</p>\n",
    "위의 식을 Largrange Multiplier를 적용하면 다음과 같이 변형할 수 있다.  \n",
    "\n",
    "<p>$$L(x,\\alpha,\\beta) = f(x)+\\alpha g(x) + \\beta h(x)$$</p>\n",
    "만약 <span>$\\alpha \\ge 0$</span>이라면 아래 식이 성립하는 것을 알 수 있다.  \n",
    "<p>$$f^{*} \\ge min_{x \\in C} L(x,\\alpha,\\beta) \\ge min_{x} L(x,\\alpha,\\beta)$$</p>\n",
    "<p>$$C: \\alpha \\ge 0 \\text{이라는 조건이 없을 경우 x의 집합}$$</p>\n",
    "<p>$$f^{*}: \\text{찾고자하는 Optimal 한 값}$$</p>\n",
    "\n",
    "**Step 2**  \n",
    "Primal -> Duality로 변형하기 위하여 Largrange Function을 미분한다.\n",
    "사용하기 위한 각각의 Function이 다음과 같을 경우  \n",
    "- <span>$f(x) = cx$</span>\n",
    "- <span>$g(x) = Gx-g \\le 0$</span>\n",
    "- <span>$h(x) = Hx-h = 0$</span>\n",
    "\n",
    "위와 같이 정리하면 식을 다음과 같이 정의할 수 있다.\n",
    "<p>$$\\frac{\\partial L}{\\partial x}= c+\\alpha G+\\beta H = 0 \\rightarrow c = -\\alpha G -\\beta H$$</p>\n",
    "\n",
    "위의 식을 맨 처음 Largrange Function에 대입하면 다음과 같은 결과가 나온다.\n",
    "<p>$$L(x,\\alpha,\\beta) = f(x)+\\alpha g(x) + \\beta h(x)$$</p>\n",
    "<p>$$= (-\\alpha G -\\beta H)x + \\alpha(Gx-g)+\\beta(Hx-h) = -\\alpha g - \\beta h$$</p>\n",
    "\n",
    "**위의 식은 더이상 <span>$x$</span>에 관한 식이 아닌 <span>$\\alpha,\\beta$</span>에 관한 식이다. 위의식 <span>$-\\alpha g - \\beta h = d(\\alpha,\\beta)$</span>라고 한다면 Largrange Function은 다음과 같다.**  \n",
    "\n",
    "**Step 3**  \n",
    "<p>$$f^{*} \\ge d(\\alpha,\\beta)$$</p>\n",
    "위의 식이 성립하므로, Optimal한 값의 Minimize하는 것은 Duality Probelm으로 나타내었을 때 Function을 Maximize하는 것과 동일하게 된다.\n",
    "\n",
    "**즉, Primal -> Duality가 되면서 Minimize가 Maximize가 되었고, x에 관한 식이 <span>$\\alpha,\\beta$</span>에 관한 식으로서 변형되었다.**  \n",
    "\n",
    "**최종적인 식은 다음과 같다.**  \n",
    "<p>$$ max_{\\alpha \\ge 0,\\beta} d(\\alpha,\\beta)$$</p>\n",
    "<p>$$\\text{s.t.  }\\alpha \\ge 0$$</p>\n",
    "\n",
    "**Duality GAP**  \n",
    "<p>$$f^{*} \\ge min_{x \\in C} L(x,\\alpha,\\beta) \\ge min_{x} L(x,\\alpha,\\beta) = d(\\alpha,\\beta)$$</p>\n",
    "\n",
    "위의 식에서 항상 등호가 성립할 수는 없다.  \n",
    "**즉, 최종적인 식(<span>$d(\\alpha,\\beta)$</span>)은 Primal Problem에서의 Lower Boundary로서 Maximization하면 비슷한 값이 나오겠다이지, 등호가 반드시 성립은 하지 않는다. 이에 따른 실제 Optimal의 값(<span>$f^{*}$</span>)과의 차이를 duality gap이라고 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Primal and Dual with KKT Condition\n",
    "**KKT Condition이란, 이러한 Condition을 만족하는 경우 위에서 언급한 Duality Gap이 없어지고 Strong Duality이 되는 조건이다.**  \n",
    "\n",
    "먼저 KKT의 조건을 살펴보면 각각 다음과 같다.  \n",
    "- <span>$\\partial L(x^{*},\\alpha^{*},\\beta^{*}) = 0$</span>: Lagrange Multiplier Condition\n",
    "- <span>$\\alpha^{*} \\ge 0$</span>: Primal -> Duality Condition\n",
    "- <span>$g(x^{*}) \\le 0$</span>: Primal Condition\n",
    "- <span>$h(x^{*}) = 0$</span>: Primal Condition\n",
    "- <span>$\\alpha^{*}g(x^{*}) = 0$</span>: **New Condition**\n",
    "\n",
    "위의 5가지 조건에서 새롭게 추가된 조건은 5번째 조건이다.  \n",
    "즉, 1~4는 기존의 Primal, Duality Problem을 해결하기 위한 Condition이었고, 5번째 새로운 Condition이 추가가 되면 어떻게 KKT Condition이 성립하여 Duality Gap이 없어지는지 알아보자.\n",
    "\n",
    "위의 Duality에서 사용한 식을 살펴보면 다음과 같다.  \n",
    "1. <span>$f^{*} \\ge min_{x \\in C} L(x,\\alpha,\\beta) \\ge min_{x} L(x,\\alpha,\\beta) = max_{\\alpha \\ge 0,\\beta} d(\\alpha,\\beta)$</span>\n",
    "2. <span>$L(x,\\alpha,\\beta) = f(x)+\\alpha g(x) + \\beta h(x)$</span>\n",
    "3. <span>$f(x^{*}) = d(\\alpha^{*},\\beta^{*})$</span>(Strong Duality라면)\n",
    "\n",
    "**Duality Problem**에서 전부 등호가 되기 위해서는 다음과 같은 조건이 성립하여야 한다.  \n",
    "<p>$$d(\\alpha^{*},\\beta^{*}) = min_{x}L(x,\\alpha^{*},\\beta^{*}) = L(x^{*},\\alpha^{*},\\beta^{*}) = f(x^{*})$$</p>\n",
    "\n",
    "**Primal Problem**에서 전부 등호가 되기 위해서는 다음과 같은 조건이 성립하여야 한다.  \n",
    "<p>$$L(x^{*},\\alpha^{*},\\beta^{*}) = f(x^{*})+\\alpha^{*} g(x^{*}) + \\beta^{*} h(x^{*}) = f(x^{*})$$</p>\n",
    "\n",
    "위의 두 식을 1번에 넣고 부등호를 유지하면 다음과 같다.  \n",
    "<p>$$L(x^{*},\\alpha^{*},\\beta^{*}) \\le f(x^{*})+\\alpha^{*} g(x^{*}) + \\beta^{*} h(x^{*}) \\le f(x^{*})$$</p>\n",
    "<p>$$\\therefore f(x^{*}) \\le f(x^{*}) +\\alpha^{*} g(x^{*}) + \\beta^{*} h(x^{*}) \\le f(x^{*})$$</p>\n",
    "\n",
    "**최종적인 식을 얻기 위해서는 부등호 사이에 있는 <span>$\\alpha^{*} g(x^{*}) + \\beta^{*} h(x^{*})$</span>값이 0이 되어야 하나, Primal Condition에서 <span>$h(x^{*})=0$</span>이라 가정하였기 때문에 <span>$\\alpha^{*} g(x^{*})$</span>을 만족하면 된다.**  \n",
    "\n",
    "즉, 위에서 새롭게 추가된 New Condition을 만족하게 되면 Primal -> Duality + Strong Duality가 만족하게 된다.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1oNZ5gyNxFMJg1TvQshCc1bY8QlvSDLJO\"><br>\n",
    "사진 참조: <a href=\"https://ratsgo.github.io/convex%20optimization/2018/01/26/KKT/\">ratsgo 블로그</a>\n",
    "\n",
    "\n",
    "### Dual Representation of SVM\n",
    "위의 Duality + KKT Condition을 SVM에 적용하면 다음과 같다.  \n",
    "\n",
    "<p>$$min_{w,b}||w||$$</p>\n",
    "<p>$$s.t(wx_j+b)y_j \\ge 1$$</p>\n",
    "\n",
    "<p>$$L(w,b,\\alpha) = \\frac{1}{2}ww-\\sum_{j}\\alpha_j[(wx_j+b)y_j-1]$$</p>\n",
    "\n",
    "위의 식에서 KKT Condition의 조건을 차례대로 적용하면 다음과 같다.  \n",
    "\n",
    "**1. <span>$\\partial L(x^{*},\\alpha^{*},\\beta^{*}) = 0$</span>: Lagrange Multiplier Condition**  \n",
    "- <span>$\\frac{\\partial L(w,b,\\alpha)}{\\partial w} \\rightarrow w = \\sum_{i=1}^{n} \\alpha_i y_i x_i$</span>\n",
    "- <span>$\\frac{\\partial L(w,b,\\alpha)}{\\partial b} \\rightarrow 0 = \\sum_{i=1}^{n} \\alpha_i y_i$</span>\n",
    "\n",
    "**2. <span>$\\alpha^{*} \\ge 0$</span>: Primal -> Duality Condition**\n",
    "<p>$$\\alpha_j \\ge 0$$</p>\n",
    "\n",
    "**3. <span>$\\alpha^{*}g(x^{*}) = 0$</span>: New Condition**\n",
    "<p>$$\\alpha_i ((wx_j+b)y_j-1)=0$$</p>\n",
    "\n",
    "**참조**  \n",
    "1. 자세한 수식 유도: <a href=\"https://kaist.edwith.org/machinelearning1_17/lecture/10605/\">문일철 머신러닝 강의</a>\n",
    "2. Error를 허용하는 SVM Dual Representation: <a href=\"https://ratsgo.github.io/convex%20optimization/2018/01/26/KKT/\">ratsgo 블로그</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Kernel\n",
    "\n",
    "Kernel이라는 것은 Mapping Function을 사용하여 Data의 차원을 늘려서 Linear하게 Classify를 못하는 문제를 Linear 하게 Classify하게 바꾼다는 의미이다.  \n",
    "\n",
    "XOR의 예시를 살펴보게 되면 다음과 같다.  \n",
    "![png](./images/15.png)\n",
    "\n",
    "Linear로서 Classify하지 못했던 문제를 Mapping Function인 <span>$\\pi(x)$</span>를 통하여 Data를 2->3차원으로서 변형시키고 Linear하게 Classify하게 한다.  \n",
    "\n",
    "이러한 간단한 Mapping Function을 사용하게 되면, Linear하게 계속하여 Space가 증가하게 된다. 따라서 이러한 Space를 작게 하고, 연산의 양을 줄이기 위하여 **Kernel을 사용하게 된다.**  \n",
    "\n",
    "먼저 Kernel의 식을 살펴보면 다음과 같다.\n",
    "<p>$$K(x_i,x_j) = \\varphi(x_i) \\cdot \\varphi(x_j)$$</p>\n",
    "\n",
    "각각의 Data의 차원을 이동하는 <span>$\\varphi$</span>의 내적으로서 표현한다는 것 이다.  \n",
    "이러한 Kernel의 대표적인 종류를 생각하면 다음과 같다.  \n",
    "\n",
    "- Polynomial: <span>$k(x_i,x_j) = (x_i \\cdot x_j)^d$</span>\n",
    "- Gaussian: <span>$k(x_i,x_j) = exp(-r||x_i - x_j||^2) (\\text{단,  }r = \\frac{1}{2 \\sigma^2})$</span>\n",
    "- Hyperbolic tangent: <span>$k(x_i,x_j) = tanh(kx_i \\cdot x_j+c) (\\text{단,  }k > 0, c<0 )$</span>\n",
    "\n",
    "위에서 Polynormial일때를 생각하면 다음과 같이 나타낼 수 있다.\n",
    "- Polynomial Function Degree 1: <span>$K(<x_1,x_2>,<z_1,z_2>) = <x_1,x_2> \\cdot <z_1,z_2> = x_1z_1+x_2z_2 = x \\cdot z$</span>\n",
    "- Polynomial Function Degree 2: <span>$K(<x_1,x_2>,<z_1,z_2>) = <x_1^2,\\sqrt{2}x_1x_2,x_2^2> \\cdot <z_1^2,\\sqrt{2}z_1z_2,z_2^2> = x_1^2z_1^2+2x_1x_2z_1z_2+x_2^2z_2^2 = (x \\cdot z)^2$</span>\n",
    "...\n",
    "\n",
    "즉, Input으로 들어가는 Vector를 내적한다음 그 값을 곱하는 형식으로서 Data의 Dimension을 늘릴 수 있다.  \n",
    "\n",
    "**이러한 특성으로서 Kernel Function은 1차원의 Data를 100차원으로 늘리는 경우 101번의 연산이 필요할 뿐이므로, 원하는 차원으로서 변형하는데 계산되는 양을 매우 줄이면서 Dimension을 늘릴 수 있다. => Dimension을 바꿈으로 인하여 Non-Linear하게 문제를 해결하는 것 처럼 만들 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8 SVM with Kernel\n",
    "위에서 최종적으로 얻은 식을 생각하면 다음과 같이 적용할 수 있다.  \n",
    "\n",
    "<p>$$w = \\sum_{i=1}^{N}\\alpha_iy_ix_i$$</p>\n",
    "<p>$$b = y_j - \\sum_{i=1}^{N} \\alpha_iy_ix_ix_j$$</p>\n",
    "<p>$$\\because y_j = w_jx_j+b_j$$</p>\n",
    "\n",
    "위의 식에서 Kernel을 적용하면 다음과 같이 나타낼 수 있다.\n",
    "<p>$$w = \\sum_{i=1}^{N}\\alpha_iy_i\\varphi(x_i)$$</p>\n",
    "<p>$$b = y_j - \\sum_{i=1}^{N} \\alpha_iy_i\\varphi(x_i)\\varphi(x_j) = y_j - \\sum_{i=1}^{N} \\alpha_iy_iK(x_i,x_j)$$</p>\n",
    "\n",
    "**위의 w는 Kernel Trick 형식이 아닌 Mapping Function이다. 하지만 우리는 결국에 SVM의 식을 생각해보자. 즉, SVM은 궁극적으로 Input에 대하여 양수인지 음수인지로 Classification하는 Model이다.**  \n",
    "\n",
    "식으로서 생각하면 다음과 같다.  \n",
    "<p>$$sign(w \\cdot x +b)$$</p>\n",
    "위의 식에 정리한 식을 대립하면 다음과 같은 결과를 얻게 된다.\n",
    "<p>$$sign(w \\cdot \\varphi(x) +b) = sign(\\sum_{i=1}^{N}\\alpha_iy_i\\varphi(x_i)\\cdot \\varphi(x) + y_j - \\sum_{i=1}^{N} \\alpha_iy_iK(x_i,x_j))$$</p>\n",
    "<p>$$= sign(\\sum_{i=1}^{N}\\alpha_iy_i K(x_i,x) + y_j - \\sum_{i=1}^{N} \\alpha_iy_iK(x_i,x_j))$$</p>\n",
    "\n",
    "즉, 실제 Model의 결과에서는 Kernel형태로 들어가게 되므로 문제없이 사용할 수 있다.\n",
    "\n",
    "**결과적으로 SVM은 Kernel Trick을 활용하여 Linear에서 해결하지 못하였던 문제를 NonLinear하게 해결할 수 있는 Classification Model로서 바뀌게 된다.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
